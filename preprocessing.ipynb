{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Finn Buchrieser, 11846398"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In this script, reddit comments is preprocessed\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both datasets are read in using the pd.read function of pandas\n",
    "\n",
    "comments_1 = pd.read_csv(r\"C:\\Users\\Finn\\Desktop\\Foundations of CSS\\Stuff\\Reddit_Posts\"\n",
    "                         r\"\\wallstreetbets_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_2 = pd.read_csv(r\"C:\\Users\\Finn\\Desktop\\Foundations of CSS\\Stuff\"\n",
    "                         r\"\\Reddit_Posts\\wallstreetbets_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only comments are kept in the df for further use, posts and unnecessary\n",
    "# columns are removed\n",
    "# all comments without dates are removed and the date is converted into\n",
    "# the datetime format\n",
    "\n",
    "comments_2 = comments_2.loc[comments_2[\"title\"] == \"Comment\"]\n",
    "comments_2.dropna(inplace = True, subset=[\"timestamp\"])\n",
    "comments_2[\"timestamp\"] = comments_2[\"timestamp\"].apply(lambda x: datetime.strptime(str(x), '%Y-%m-%d %H:%M:%S'))\n",
    "comments_2 = comments_2[[\"id\", \"body\", \"timestamp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_1 = comments_1[[\"id\", \"body\", \"created_utc\"]]\n",
    "comments_1.dropna(inplace = True, subset=[\"created_utc\"])\n",
    "comments_1[\"created_utc\"] = comments_1[\"created_utc\"].apply(lambda x: datetime.fromtimestamp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both datasets are combined\n",
    "\n",
    "comments = pd.concat([comments_1, comments_1.rename({\"timestamp\":\"created_utc\"})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords, unnecessary punctuation and extremely long words\n",
    "# are removed, also comments with less than 3 or more than 150\n",
    "# words are removed\n",
    "\n",
    "# Laden der Stop-Wörter aus der Standardbibliothek von NLTK\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Funktion zum Bereinigen des Texts\n",
    "def clean_text(text):\n",
    "    # Entfernen von \"&\" und \"%\" Zeichen\n",
    "    text = re.sub(r'[&%]', '', text)\n",
    "    # Tokenisierung des Textes\n",
    "    tokens = text.split()\n",
    "    # Entfernen von Stop-Wörtern\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Entfernen von Wörtern, die länger als 15 Zeichen sind\n",
    "    tokens = [token for token in tokens if len(token) <= 15]\n",
    "    # Zusammensetzen der Token zu einem Text\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "# Bereinigen des Textes\n",
    "comments['cleaned_text'] = comments['body'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Entfernen von Posts, die länger als 150 Wörter sind\n",
    "comments = comments[comments['cleaned_text'].apply(lambda x: len(x.split()) <= 150)]\n",
    "comments = comments[comments['cleaned_text'].apply(lambda x: len(x.split()) >= 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is saved in a csv fiel for further sentiment analysis\n",
    "\n",
    "comments.to_csv(r\"C:\\Users\\Finn\\Desktop\\CSS_Project\"\n",
    "                r\"\\Reddit_Posts\\preprocessed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
