{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbb5248e",
   "metadata": {},
   "source": [
    "by Antonia Kraft, matr.-nr.: 11731292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00e52e-6f14-4554-bf4c-22c62f8c928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import csv\n",
    "import math\n",
    "import json\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.dates import MonthLocator\n",
    "import scipy.stats as stats\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from mlxtend.plotting import scatterplotmatrix\n",
    "from mlxtend.plotting import heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9920f81a-5187-4db8-9b2a-06909c832149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in reddit data\n",
    "redditdf = pd.read_csv(\"reddit_posts.csv\")\n",
    "redditdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b499ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in yahoo fin data\n",
    "findf = pd.read_csv(\"Finance_Data.csv\")\n",
    "\n",
    "findf = findf.sort_values(by = \"Date\", ascending=True)\n",
    "findf[\"Date\"] = pd.to_datetime(findf[\"Date\"]) \n",
    "\n",
    "findf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b6477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep data for regression\n",
    "redditdf[\"timestamp\"] = pd.to_datetime(redditdf[\"timestamp\"])\n",
    "redditdf[\"timestamp\"] = redditdf[\"timestamp\"].dt.date\n",
    "redditdf = redditdf.sort_values(by = \"timestamp\", ascending = False)\n",
    "\n",
    "redditdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ef206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataprep\n",
    "findf = pd.DataFrame(findf)\n",
    "\n",
    "findf = findf.pivot_table(index = \"Date\", columns = \"Symbol\", values = \"Value\")\n",
    "findf.reset_index(inplace = True)\n",
    "\n",
    "findf[\"Date\"] = pd.to_datetime(findf[\"Date\"])\n",
    "findf[\"Date\"] = findf[\"Date\"].dt.date\n",
    "findf = findf.sort_values(by = \"Date\", ascending = False)\n",
    "\n",
    "findf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis finito\n",
    "redditdf['VADER_class'] = redditdf['compound'].apply(lambda x: \"Positive\" if x > 0.5 else \"Negative\" if x < -0.5 else \"Neutral\")\n",
    "#print(redditdf['VADER_class'])\n",
    "\n",
    "N = len(redditdf)\n",
    "\n",
    "all = redditdf['VADER_class'].value_counts()\n",
    "pos = redditdf[\"VADER_class\"][redditdf[\"VADER_class\"] == \"Positive\"].value_counts()\n",
    "neg = redditdf[\"VADER_class\"][redditdf[\"VADER_class\"] == \"Negative\"].value_counts()\n",
    "neu = redditdf[\"VADER_class\"][redditdf[\"VADER_class\"] == \"Neutral\"].value_counts()\n",
    "\n",
    "\n",
    "all1 = pd.DataFrame({\"class\": [\"Positive\", \"Neutral\", \"Negative\"],\n",
    "                     \"num\": [(int(pos)/N)*100, (int(neu)/N)*100, (int(neg)/N)*100]})\n",
    "\n",
    "#absolute val\n",
    "sns.barplot(x = \"class\", y = \"num\", color = \"blue\", data = all1)\n",
    "plt.ylabel(\"%\")\n",
    "plt.xlabel(\"classes\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\tPositive in %: \", (pos/N)*100, \"\\n\\tNegative in %: \", (neg/N)*100, \"\\n\\tNeutral in %: \", (neu/N)*100)\n",
    "\n",
    "redditdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a9a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices over time\n",
    "sym1 = findf[\"GME\"]\n",
    "sym2 = findf[\"MSCI_WORLD\"]\n",
    "sym3 = findf[\"SPY\"]\n",
    "\n",
    "#timeline\n",
    "x_axis1 = findf[\"Date\"] \n",
    "\n",
    "#plot -> indices\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis1, sym1, label = \"GME\", color = \"orchid\", linewidth = 2)\n",
    "ax.plot(x_axis1, sym2, label = \"MSCI-World\", color = \"crimson\", linewidth = 2)\n",
    "ax.plot(x_axis1, sym3, label = \"SPY\", color = \"palegreen\", linewidth = 2)\n",
    "plt.ylabel(\"in US$\")\n",
    "plt.xlabel(\"timeline\")\n",
    "\n",
    "#stepsize of date = 2months\n",
    "loc = MonthLocator(interval=2)\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4175d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping values\n",
    "daily_class = redditdf.groupby([\"timestamp\", \"VADER_class\"]).size().unstack(fill_value=0)\n",
    "daily_class = daily_class.astype(int)\n",
    "daily_class.reset_index(inplace = True)\n",
    "pos = (daily_class[\"Positive\"])/N*100\n",
    "neu = (daily_class[\"Neutral\"])/N*100\n",
    "neg = (daily_class[\"Negative\"])/N*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the csv-files\n",
    "merge_df = pd.merge(redditdf, findf, left_on = \"timestamp\", right_on = \"Date\")\n",
    "\n",
    "#merge_df.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ae07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating column \"days\" to calculate with --> Day0 = 2020-12-08; Day752 = 2022-12-30 \n",
    "merge_df[\"delta\"] = pd.to_numeric((merge_df[\"timestamp\"] - min(merge_df[\"timestamp\"])).dt.days, downcast = \"integer\")\n",
    "merge_df[\"pos\"] = pd.to_numeric(merge_df[\"pos\"]).astype(float)\n",
    "merge_df[\"neu\"] = pd.to_numeric(merge_df[\"neu\"]).astype(float)\n",
    "merge_df[\"neg\"] = pd.to_numeric(merge_df[\"neg\"]).astype(float)\n",
    "merge_df = merge_df.dropna()\n",
    "\n",
    "merge_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1273c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the regression operators:\n",
    "#features = list(merge_df.columns[\"delta\", \"neg\", \"neu\", \"pos\", \"compound\"])\n",
    "df = pd.DataFrame().assign(Delta = merge_df[\"delta\"], Compound = merge_df[\"compound\"]).dropna()\n",
    "features = list(df.dropna())\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422a6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9955c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"neg\", \"neu\", \"pos\", \"compound\", \"GME\", \"MSCI_WORLD\", \"SPY\", \"delta\"]\n",
    "cm = np.corrcoef(merge_df[columns].values.T)\n",
    "hm = heatmap (cm, row_names=columns, column_names=columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df)\n",
    "#print(features) --> just print it, if your PC can handle that. Otherwise: don't do it.\n",
    "\n",
    "target1 = \"GME\"\n",
    "target2 = \"MSCI_WORLD\"\n",
    "target3 = \"SPY\"\n",
    "#target1.reset_index(inplace = True)\n",
    "#target2.reset_index(inplace = True)\n",
    "#target3.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing data\n",
    "def get_data(df, features, normalize = False):\n",
    "    X = df.loc[:, features]\n",
    "    if not normalize:\n",
    "        return X.to_numpy()\n",
    "    return preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = get_data(df, features, normalize = True)\n",
    "X = get_data(df, features)\n",
    "\n",
    "y1_norm = get_data(merge_df, target1, normalize = True)\n",
    "y1 = get_data(merge_df, target1)\n",
    "\n",
    "y2_norm = get_data(merge_df, target2, normalize = True)\n",
    "y2 = get_data(merge_df, target2)\n",
    "\n",
    "y3_norm = get_data(merge_df, target3, normalize = True)\n",
    "y3 = get_data(merge_df, target3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbbf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplots\n",
    "columns = [\"compound\", \"delta\", \"GME\", \"MSCI_WORLD\", \"SPY\"]\n",
    "\n",
    "scatterplotmatrix(merge_df[columns].values, figsize = (15,10), names = columns)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b02ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE (mean squared error), bias, variance using k-fold (k=5) for GME\n",
    "lam = 1.0\n",
    "reg = Ridge(alpha = 1.0)\n",
    "cv = KFold(n_splits=5)\n",
    "\n",
    "def cross_validate(X, y1, lam=1.0):\n",
    "    mse1 = []\n",
    "    bias1 = []\n",
    "    variance1 = []\n",
    "    #splitting the data to 5 parts -> 1 test dataset + 4 training datasets. Therefore I can check how consistent it is.\n",
    "    \n",
    "\n",
    "    #performing cross validation + getting y-mean + beta-mean\n",
    "    reg = Ridge(alpha=lam)\n",
    "    for train, test in cv.split(X):\n",
    "        X_train, X_test, y1_train, y1_test = X[train], X[test], y1[train], y1[test]\n",
    "        \n",
    "        m = reg.fit(X_train, y1_train)\n",
    "        y1_hat = reg.predict(X_test)\n",
    "        beta = reg.coef_\n",
    "        mse1.append(mean_squared_error(y1_test, y1_hat))\n",
    "        bias1.append(np.mean(y1_hat - y1_test)**2)\n",
    "        variance1.append(np.var(y1_hat))\n",
    "    print(np.mean(y1_hat), \"\\n\", np.mean(beta))\n",
    "    \n",
    "    return(np.mean(mse1), np.mean(bias1), np.mean(variance1))\n",
    "\n",
    "#k-fold cross validation with lambda = 1.0\n",
    "mse1, bias1, variance1 = cross_validate(X_norm, y1_norm, 1)\n",
    "\n",
    "print\n",
    "\n",
    "print(\"GME\\nfor lambda = 1.0: \\n\\tmse=%f \\n\\tbias=%f \\n\\tvariance=%f\"%(mse1,bias1,variance1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv + reg\n",
    "reg.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE (mean squared error), bias, variance using k-fold (k=5) for MSCI-World\n",
    "\n",
    "def cross_validate(X, y2, lam=1.0):\n",
    "    mse2 = []\n",
    "    bias2 = []\n",
    "    var2 = []\n",
    "    #splitting the data to 5 parts -> 1 test dataset + 4 training datasets. Therefore I can check how consistent it is.\n",
    "\n",
    "    #performing cross validation + getting y-mean + beta-mean\n",
    "    reg = Ridge(alpha=lam)\n",
    "    for train, test in cv.split(X):\n",
    "        X_train, X_test, y2_train, y2_test = X[train], X[test], y2[train], y2[test]\n",
    "        \n",
    "        reg.fit(X_train, y2_train)\n",
    "        y2_hat = reg.predict(X_test)\n",
    "        beta = reg.coef_\n",
    "\n",
    "        mse2.append(mean_squared_error(y2_test, y2_hat))\n",
    "        bias2.append(np.mean(y2_hat - y2_test)**2)\n",
    "        var2.append(np.var(y2_hat))\n",
    "        \n",
    "        \n",
    "    print(np.mean(y2_hat), \"\\n\", np.mean(beta))\n",
    "    return(np.mean(mse2), np.mean(bias2), np.mean(var2))\n",
    "\n",
    "#k-fold cross validation with lambda = 1.0\n",
    "mse2, bias2, variance2 = cross_validate(X_norm, y2_norm, 1.0)\n",
    "\n",
    "print(\"MSCI_WORLD\\nfor lambda = 1.0: \\n\\tmse=%f \\n\\tbias=%f \\n\\tvariance=%f\"%(mse2,bias2,variance2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE (mean squared error), bias, variance using k-fold (k=5) for SPY\n",
    "\n",
    "def cross_validate(X, y3, lam=1.0):\n",
    "    mse3 = []\n",
    "    bias3 = []\n",
    "    var3 = []\n",
    "    #splitting the data to 5 parts -> 1 test dataset + 4 training datasets. Therefore I can check how consistent it is.\n",
    "\n",
    "    #performing cross validation + getting y-mean + beta-mean\n",
    "    reg = Ridge(alpha=lam)\n",
    "    for train, test in cv.split(X):\n",
    "        X_train, X_test, y3_train, y3_test = X[train], X[test], y3[train], y3[test]\n",
    "\n",
    "        reg.fit(X_train, y3_train)\n",
    "        y3_hat = reg.predict(X_test)\n",
    "        beta = reg.coef_\n",
    "\n",
    "        mse3.append(mean_squared_error(y3_test, y3_hat))\n",
    "        bias3.append(np.mean(y3_hat - y3_test)**2)\n",
    "        var3.append(np.var(y3_hat))\n",
    "    print(np.mean(y3_hat), \"\\n\", np.mean(beta))\n",
    "    return(np.mean(mse3), np.mean(bias3), np.mean(var3))\n",
    "\n",
    "#k-fold cross validation with lambda = 1.0\n",
    "mse3, bias3, variance3 = cross_validate(X_norm, y3_norm, 1.0)\n",
    "\n",
    "print(\"SPY: \\nfor lambda = 1.0: \\n\\tmse=%f \\n\\tbias=%f \\n\\tvariance=%f\"%(mse3,bias3,variance3,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a1b8c5b35de44467827c3de88570f1eeea6a25114d6990bc237882239df6be5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
