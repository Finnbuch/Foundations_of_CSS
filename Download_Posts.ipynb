{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "created by Finn Buchrieser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the first part of this script posts\n",
    "# from a single subreddit can be download\n",
    "\n",
    "from numpy.core.numeric import NaN\n",
    "import pandas as pd\n",
    "from pmaw import PushshiftAPI\n",
    "import time\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API object is created and the number of times\n",
    "# the script is repeated can be set (will be explained\n",
    "# in further comments)\n",
    "\n",
    "api = PushshiftAPI()\n",
    "posts_df = NaN\n",
    "number_of_repetition = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# in the following loop post will be downloaded\n",
    "\n",
    "for i, run in enumerate(range(number_of_repetition)):\n",
    "\n",
    "    # before and after can be used to select the start\n",
    "    # and end date for the downloads\n",
    "\n",
    "    before = int(dt.datetime(2022,12,30,0,0).timestamp())\n",
    "    after = int(dt.datetime(2018,1,1,0,0).timestamp())\n",
    "    print(i)\n",
    "\n",
    "    # here the name of the subreddit can be entered\n",
    "\n",
    "    subreddit = \"funny\"\n",
    "\n",
    "    # here the maximum amount of post that should be downloaded can\n",
    "    # be entered\n",
    "\n",
    "    limit = 5000000\n",
    "\n",
    "    # in the following try block the post will be downloaded\n",
    "    # if the api crashed serversided or the download fails for \n",
    "    # other reasons the porcess will be repeateated the amount of times\n",
    "    # entered in number of repetiotions, only posts that werent previously\n",
    "    # downloaded will be downloaded in further attempts\n",
    "    \n",
    "    try:\n",
    "        if i != 0 :\n",
    "           after = max(posts_df[\"created_utc\"])\n",
    "           print(after)\n",
    "        posts = api.search_submissions(subreddit=subreddit,\n",
    "                                       size = limit, before = before, after = after)\n",
    "        if i == 0:\n",
    "            posts_df = pd.DataFrame(posts)\n",
    "        else:\n",
    "            posts_df = pd.concat([posts_df, pd.DataFrame(posts)], sort = False)\n",
    "    except:\n",
    "        print(f'run {i} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the data is written to a csv file\n",
    "posts_df.to_csv(r'C:\\Users\\Finn\\Desktop\\Reddit_scraping\\wsb_posts.csv',\n",
    "                header = True, index = False, columns = list(posts_df.axes[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the following part of the script the posts or comments from multiple subreddits\n",
    "# can be downloaded at once\n",
    "\n",
    "posts_df = NaN\n",
    "# the number of workers is used to select how many threads should work in parallel\n",
    "api = PushshiftAPI(num_workers = 10)\n",
    "\n",
    "before = int(dt.datetime(2022,10,30,0,0).timestamp())\n",
    "after = int(dt.datetime(2022,10,4,0,0).timestamp())\n",
    "\n",
    "# enter the subreddits you want to download here\n",
    "\n",
    "subreddit_list = [\"Socialism\", \"neoliberal\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the following block, posts can be downloaded\n",
    "\n",
    "for item in subreddit_list :\n",
    "\n",
    "    try:\n",
    "        subreddit = item\n",
    "        limit = 5000000\n",
    "        posts = api.search_submissions(subreddit=subreddit, size = limit,\n",
    "                                       before = before, after = after, q = \"*\")\n",
    "        print(f'Retrieved {len(posts)} posts from Pushshift')\n",
    "        posts_df = pd.DataFrame(posts)\n",
    "\n",
    "        # just enter the path to the folder you want ot save the files into, the names will\n",
    "        # be generated based on the subreddit name\n",
    "\n",
    "        posts_df.to_csv(r'C:\\Users\\Finn\\Desktop\\Reddit_scraping\\\\' + item + '_posts.csv',\n",
    "                        header = True, index = False, columns=list(posts_df.axes[1]))\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 19175 posts from Pushshift\n",
      "Retrieved 47000 posts from Pushshift\n"
     ]
    }
   ],
   "source": [
    "# in the following block, comments can be downloaded\n",
    "\n",
    "for item in subreddit_list :\n",
    "    try:\n",
    "        subreddit = item\n",
    "        limit=500\n",
    "        comments = api.search_comments(subreddit = subreddit, size = limit,\n",
    "                                       before = before, after = after, q = \"*\", num_workers = 14)\n",
    "        \n",
    "        print(f'Retrieved {len(comments)} posts from Pushshift')\n",
    "        comments_df = pd.DataFrame(comments)\n",
    "\n",
    "        # just enter the path to the folder you want ot save the files into, the names will\n",
    "        # be generated based on the subreddit name\n",
    "\n",
    "        comments_df.to_csv('C:\\\\Users\\\\Finn\\\\Desktop\\\\Reddit_scraping\\\\' + item + '_comments.csv',\n",
    "                           header=True, index = False, columns = list(posts_df.axes[1]))\n",
    "    except:\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c9e49ae4375f933fe0d655a1355df9218316e054c1c063a8814bcedabbb5d08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
